{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c00bf530",
   "metadata": {},
   "source": [
    "# Project 2: Classification\n",
    "\n",
    "This project asks you to perform various experiments with classification. The dataset we are using is a toy dataset for credit card fraud detection:\n",
    "\n",
    "https://www.kaggle.com/datasets/shubhamjoshi2130of/abstract-data-set-for-credit-card-fraud-detection\n",
    "\n",
    "You will write code and discussion texts into code and text cells in this notebook. \n",
    "\n",
    "If a block starts with TODO:, this means that you need to write something there. \n",
    "\n",
    "Some code had been written for you to guide the project. Don't change the already written code.\n",
    "\n",
    "## Grading\n",
    "The points add up to 40, that is 30 + 10 bonus points. While there is no difference between the regular and the bonus points, I recommend that you solve the problems labeled \"BONUS\" after you finished the other ones. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "a440a191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691d1a9f",
   "metadata": {},
   "source": [
    "## Setup for the project\n",
    "\n",
    "Here we load the dataset, and create the training and test datasets as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "902ccefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows 3075\n",
      "The columns of the database Index(['Merchant_id', 'Transaction date', 'Average Amount/transaction/day',\n",
      "       'Transaction_amount', 'Is declined', 'Total Number of declines/day',\n",
      "       'isForeignTransaction', 'isHighRiskCountry', 'Daily_chargeback_avg_amt',\n",
      "       '6_month_avg_chbk_amt', '6-month_chbk_freq', 'isFradulent'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "isFradulent\n",
       "False    2627\n",
       "True      448\n",
       "dtype: int64"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"creditcard.csv\",  true_values=[\"Y\"], false_values=[\"N\"])\n",
    "print(f\"Number of rows {len(df.index)}\")\n",
    "print(f\"The columns of the database {df.columns}\")\n",
    "df.value_counts(\"isFradulent\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "54bf2ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xfields = [\n",
    "    'Average Amount/transaction/day',\n",
    "       'Transaction_amount', 'Is declined', 'Total Number of declines/day',\n",
    "       'isForeignTransaction', 'isHighRiskCountry', 'Daily_chargeback_avg_amt',\n",
    "       '6_month_avg_chbk_amt', '6-month_chbk_freq']\n",
    "\n",
    "df = df.replace({'Y': 1, 'N': -1})       #converting y and n to binary\n",
    "\n",
    "df_shuffled = df.sample(frac=1) # shuffle the rows\n",
    "x = df_shuffled[xfields].to_numpy(dtype=np.float64)\n",
    "y = df_shuffled[\"isFradulent\"].to_numpy(dtype=np.float64)\n",
    "# the training data is the first 2000 rows, after shuffled\n",
    "training_data_x = x[:2000]\n",
    "training_data_y = y[:2000]\n",
    "# the test data is the remaining\n",
    "test_data_x = x[2000:]\n",
    "test_data_y = y[2000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "959fcb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run this to help you with what number goes with what field:\n",
      "0 = Average Amount/transaction/day\n",
      "1 = Transaction_amount\n",
      "2 = Is declined\n",
      "3 = Total Number of declines/day\n",
      "4 = isForeignTransaction\n",
      "5 = isHighRiskCountry\n",
      "6 = Daily_chargeback_avg_amt\n",
      "7 = 6_month_avg_chbk_amt\n",
      "8 = 6-month_chbk_freq\n"
     ]
    }
   ],
   "source": [
    "print(\"Run this to help you with what number goes with what field:\")\n",
    "for i, x in enumerate(xfields):\n",
    "    print(f\"{i} = {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba69479d",
   "metadata": {},
   "source": [
    "## P1: Create an accuracy metric (7 pts)\n",
    "Create a simple accuracy metric function which for a pair of ground truth values $y$ and estimates $\\hat{y}$ (both of them arrays) calculates the accuracy of the estimate $\\hat{y}$. For instance, if you pass y = [1, 0, 1] and \n",
    "yhat = [1, 1, 0], the loss function should return 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "9bd1d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, yhat):\n",
    "    ## implement here\n",
    "    total = 0\n",
    "    for i in range(len(y)):\n",
    "        total += (int(y[i]) ^ yhat[i])/(len(y))\n",
    "    total = 1 - total\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "a577d090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.33333333333333337\n"
     ]
    }
   ],
   "source": [
    "# test your function here\n",
    "acc = accuracy([1, 0, 1], [1, 1, 0])\n",
    "print(f\"Accuracy is {acc}\") # should print 0.33..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564999e3",
   "metadata": {},
   "source": [
    "## P2: Implement a majority classifier (7 pts)\n",
    "This classifier will always return the most likely value. Training the classifier means determining what is the most likely value (regardless vhat value you pass to it). For instance, if more than half of the transactions are fraudulent, then you just return fraudulent always. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "7207afba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_majority(x, theta):\n",
    "    # whatever the value of x, we return the theta\n",
    "\n",
    "    ones = [1] * len(x)\n",
    "    if accuracy(training_data_y, ones) >= .5: return 1\n",
    "    else: return 0\n",
    "\n",
    "            \n",
    "\n",
    "# TODO: implement the train majority function\n",
    "def train_majority(training_x, training_y):\n",
    "    # this function will have to determine which is more likely to \n",
    "    # be the value of y, one (true) or zero (false)\n",
    "    return classify_majority(training_x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "d61379a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8651162790697672\n"
     ]
    }
   ],
   "source": [
    "# TODO: use the train_majority function to find the theta value for the training dataset\n",
    "theta = 0\n",
    "\n",
    "theta = train_majority(training_data_x, training_data_y)\n",
    "\n",
    "# TODO: now use the theta value to create the test_data_yhat array which contains the classification for each test value \n",
    "\n",
    "test_data_yhat = [theta] * (len(test_data_y))\n",
    "\n",
    "# TODO: now calculate the accuracy of the classifier using the function implemented in P1, and print it out\n",
    "\n",
    "print(accuracy(test_data_y, test_data_yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc53d27d",
   "metadata": {},
   "source": [
    "TODO: Discuss here the performance of the majority classifier. Would this beat a classifier that just returns random values? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "eda28ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This would beat a classifier that just returns random values in most cases because the average accuracy of a random classifier is 50 percent\n",
      "where the majority classifier is always at least above 50 percent there may be instances where a random classifier outerforms the majority \n",
      "classifier, but that possibility diminishes with scale\n"
     ]
    }
   ],
   "source": [
    "print(\"This would beat a classifier that just returns random values in most cases because the average accuracy of a random classifier is 50 percent\\nwhere the majority classifier is always at least above 50 percent there may be instances where a random classifier outerforms the majority \\nclassifier, but that possibility diminishes with scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c63bf75",
   "metadata": {},
   "source": [
    "## P3: Implement a hand engineered classifier (8 pts)\n",
    "\n",
    "Engineer by hand a classifier function that predicts whether  a transaction is  fraudulent or not. Your function should have a $\\theta$ parameter which allows you to tweak it. \n",
    "The problem requires you to design a function that performs this classification, tweak its parameters, and measure its accuracy for the best parametrization you found. You should aim for a function that, at minimum, performs better than the majority classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "a7d91324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement here your hand-engineered classifier\n",
    "# The example below is just a very bad example, but it gives you an idea of how you can reason about the classification problem.\n",
    "# In your implementation, you should try to actually find some kind of clever algorithm. You can also use more complex parametrizations\n",
    "\n",
    "class Classifiers:\n",
    "        transaction_amount = 0.0\n",
    "        high_risk = 1\n",
    "        declines = 0.0\n",
    "        chargebacks = 0.0\n",
    "\n",
    "def predict_handwritten(x,theta):\n",
    "    y_pred = [0] * len(x)\n",
    "    for i in range(len(x)):\n",
    "        if(x[i,1] > theta.transaction_amount):\n",
    "            y_pred[i] = 1\n",
    "            continue\n",
    "        elif(x[i,5]):\n",
    "            y_pred[i] = 1\n",
    "            continue\n",
    "        elif(x[i,3] > theta.declines):\n",
    "            y_pred[i] = 1\n",
    "            continue\n",
    "        elif(x[i,8] > theta.chargebacks):\n",
    "            y_pred[i] = 1\n",
    "            continue\n",
    "        else:\n",
    "            y_pred[i] = 0\n",
    "\n",
    "    return y_pred\n",
    "    \n",
    "def transaction_classifier(x,y):\n",
    "\n",
    "    \n",
    "    k = 0\n",
    "    j = 0\n",
    "    lowest = 999999999\n",
    "    highest = 0\n",
    "    amount = 0\n",
    "    \n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        if 0 == y[i]:\n",
    "            if x[i,1] < lowest:\n",
    "                lowest = x[i,1]\n",
    "            if x[i,1] > highest:\n",
    "                highest = x[i,1]\n",
    "            amount+= x[i,1]\n",
    "            k+=1\n",
    "       \n",
    "    valid_mean = amount/k\n",
    "\n",
    "    k = 0\n",
    "    \n",
    "    for k in range (int(highest)): \n",
    "        top = ((highest - k)-valid_mean)/(highest - lowest)\n",
    "        if top < .75: break\n",
    "\n",
    "    return ((highest-k))\n",
    "\n",
    "def decline_classifier(x,y):\n",
    "\n",
    "    \n",
    "    k = 0\n",
    "    j = 0\n",
    "    lowest = 999999999\n",
    "    highest = 0\n",
    "    amount = 0\n",
    "    \n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        if 0 == y[i]:\n",
    "            if x[i,3] < lowest:\n",
    "                lowest = x[i,3]\n",
    "            if x[i,3] > highest:\n",
    "                highest = x[i,3]\n",
    "            amount+= x[i,3]\n",
    "            k+=1\n",
    "       \n",
    "    valid_mean = amount/k\n",
    "\n",
    "    k = 0\n",
    "    \n",
    "    for k in range (int(highest)): \n",
    "        top = ((highest - k)-valid_mean)/(highest - lowest)\n",
    "        if top < .75: break\n",
    "\n",
    "    return ((highest-k))\n",
    "\n",
    "def chargeback_classifier(x,y):\n",
    "\n",
    "    \n",
    "    k = 0\n",
    "    j = 0\n",
    "    lowest = 999999999\n",
    "    highest = 0\n",
    "    amount = 0\n",
    "    \n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        if 0 == y[i]:\n",
    "            if x[i,8] < lowest:\n",
    "                lowest = x[i,8]\n",
    "            if x[i,8] > highest:\n",
    "                highest = x[i,8]\n",
    "            amount+= x[i,8]\n",
    "            k+=1\n",
    "       \n",
    "    valid_mean = amount/k\n",
    "    \n",
    "    k = 0\n",
    "    \n",
    "    for k in range (int(highest)): \n",
    "        top = ((highest - k)-valid_mean)/(highest - lowest)\n",
    "        if top < .6: break\n",
    "\n",
    "    return ((highest-k))\n",
    "\n",
    "    \n",
    "\n",
    "def classify_handwritten(x, y, depth):\n",
    "\n",
    "    classifiers = Classifiers()\n",
    "\n",
    "    if (depth <= 0) | (len(x) <= 1):\n",
    "        classifiers.transaction_amount = transaction_classifier(x,y)\n",
    "        classifiers.declines = decline_classifier(x,y)\n",
    "        classifiers.chargebacks = chargeback_classifier(x,y)\n",
    "    \n",
    "        return classifiers\n",
    "\n",
    "    \n",
    "    sub_left  = classify_handwritten(x[:len(x)//2], y[:len(x)//2], depth - 1)\n",
    "    sub_right = classify_handwritten(x[len(x)//2:], y[len(x)//2:], depth - 1)\n",
    "\n",
    "    y_left  = predict_handwritten(x[:len(x)//2], sub_left)\n",
    "    y_right = predict_handwritten(x[len(x)//2:], sub_right)\n",
    "\n",
    "    left_accuracy = accuracy(y[:len(x)//2], y_left)\n",
    "    right_accuracy = accuracy(y[len(x)//2:], y_right)\n",
    "\n",
    "    if right_accuracy > left_accuracy:\n",
    "        return sub_right\n",
    "\n",
    "    return sub_left\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "a723623d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9776744186046512\n"
     ]
    }
   ],
   "source": [
    "# TODO: Now, run some experiments with your function. Experiment with different values of the parameter theta. \n",
    "theta = Classifiers\n",
    "\n",
    "depth = 0       #for my classify_handwritten function I'm using depth as the tweakable parameter instead of theta\n",
    "                #depth can be any number 0-9 best accuracy is 0 and 1\n",
    "\n",
    "theta = classify_handwritten(training_data_x, training_data_y, depth)\n",
    "\n",
    "y_pred = predict_handwritten(test_data_x, theta)\n",
    "\n",
    "print(accuracy(test_data_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "86997918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9776744186046512\n"
     ]
    }
   ],
   "source": [
    "# TODO: calculate the accuracy of the classifier on the test data with the best\n",
    "# theta found above and print it.\n",
    "\n",
    "theta = classify_handwritten(training_data_x, training_data_y, 0)\n",
    "\n",
    "y_pred = predict_handwritten(test_data_x, theta)\n",
    "\n",
    "print(accuracy(test_data_y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c319f058",
   "metadata": {},
   "source": [
    "TODO: Describe in one paragraph your experiments and evaluation. Discuss the overall accuracy your classifier. Did you manage to beat the \"majority\" classifier? Comment on how easy or hard was to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "b705483d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I spent a ton of time looking through the data to figure out the threshold for fraudulent transactions for each variable.\n",
      "I then found a way to approximate those thresholds by finding the upper quartile of the nonfraudulent transactions.\n",
      "Setting all transactions over the thresholds to fraudulent resulted in a .979 accuracy beating the majority classifier.\n",
      "Making the programs to find the upper quartiles for each variable wasn't that hard but I had to reference the textbook \n",
      "to figure out how to add the depth parameter. Unfortunately I couldn't implement this into just one function.\n"
     ]
    }
   ],
   "source": [
    "print(\"I spent a ton of time looking through the data to figure out the threshold for fraudulent transactions for each variable.\")\n",
    "print(\"I then found a way to approximate those thresholds by finding the upper quartile of the nonfraudulent transactions.\")\n",
    "print(\"Setting all transactions over the thresholds to fraudulent resulted in a .979 accuracy beating the majority classifier.\")\n",
    "print(\"Making the programs to find the upper quartiles for each variable wasn't that hard but I had to reference the textbook \\nto figure out how to add the depth parameter. Unfortunately I couldn't implement this into just one function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0548dea",
   "metadata": {},
   "source": [
    "## P4: Implement a logistic regression classifier using sklearn (8 pts)\n",
    "Implement a logistic regression function using the sklearn library. \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "eca94c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,) (2000, 5)\n",
      "[0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#data for sklearn funcitons\n",
    "\n",
    "theta = classify_handwritten(training_data_x, training_data_y, 0)\n",
    "\n",
    "i = 0\n",
    "\n",
    "for i in range(len(training_data_x)):\n",
    "    if (float(training_data_x[i][1]) > theta.transaction_amount): training_data_x[i][1] = 1\n",
    "    else: training_data_x[i][1] = 0\n",
    "\n",
    "    if (training_data_x[i][8] > theta.chargebacks): training_data_x[i][8] = 1\n",
    "    else: training_data_x[i][8] = 0\n",
    "\n",
    "    if (training_data_x[i][3] > theta.declines): training_data_x[i][3] = 1\n",
    "    else: training_data_x[i][3] = 0\n",
    "\n",
    "i = 0\n",
    "\n",
    "for i in range(len(test_data_x)):\n",
    "    if (float(test_data_x[i][1]) > theta.transaction_amount): test_data_x[i][1] = 1\n",
    "    else: test_data_x[i][1] = 0\n",
    "\n",
    "    if (test_data_x[i][8] > theta.chargebacks): test_data_x[i][8] = 1\n",
    "    else: test_data_x[i][8] = 0\n",
    "\n",
    "    if (test_data_x[i][3] > theta.declines): test_data_x[i][3] = 1\n",
    "    else: test_data_x[i][3] = 0\n",
    "\n",
    "print(np.shape(training_data_x[:,8]), np.shape(training_data_x[:,1:6]))\n",
    "\n",
    "training_data_x = np.append(training_data_x[:,1:6], np.array(training_data_x[:,8]).reshape(-1,1), axis = 1)\n",
    "test_data_x = np.append(test_data_x[:,1:6], np.array(test_data_x[:,8]).reshape(-1,1), axis = 1)\n",
    "\n",
    "print(training_data_x[:][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "1b91956b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9767441860465116\n"
     ]
    }
   ],
   "source": [
    "# TODO: implement the logistic regression here in a function \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "log = LogisticRegression()\n",
    "\n",
    "log.fit(training_data_x,training_data_y)\n",
    "\n",
    "y_pred = log.predict(test_data_x)\n",
    "\n",
    "print(accuracy_score(test_data_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "7af53924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9767441860465116\n"
     ]
    }
   ],
   "source": [
    "# TODO: now, run some experiments with it, and measure the accuracy with various parametrizations. In particular, you should run it with and without regularization. \n",
    "# In the last line, print the accuracy with the best parameters.\n",
    "\n",
    "log = LogisticRegression(max_iter = 100, class_weight = {0:1, 1:1, 2:1, 3:1, 4:1, 5:1,}, random_state=1100000)\n",
    "\n",
    "log.fit(training_data_x,training_data_y)\n",
    "\n",
    "y_pred = log.predict(test_data_x)\n",
    "\n",
    "print(accuracy_score(test_data_y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b3bfac",
   "metadata": {},
   "source": [
    "\n",
    "TODO: Describe in one paragraph your experiments and evaluation of the Logistic Regression classifier. Consider things such as accuracy, training time, ease of tweaking of the parameters. Compare it with the accuracy of the hand-engineered classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "657e4dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When experimenting I found that the only variable to effect the accuracy was index 1 and 0, if its weight was set to 0 the classifier would default to the majority classifier\n",
      "and if it was set too large it would bring the accuracy down to about 14 percent it scored about the same as my classifier\n"
     ]
    }
   ],
   "source": [
    "print(\"When experimenting I found that the only variable to effect the accuracy was index 1 and 0, if its weight was set to 0 the classifier would default to the majority classifier\\nand if it was set too large it would bring the accuracy down to about 14 percent it scored about the same as my classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fcd1c6",
   "metadata": {},
   "source": [
    "## P5 Bonus: Implement a random forest classifier using sklearn (5 pts)\n",
    "Implement a random forest classifier using sklearn \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "9663e31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9776744186046512\n"
     ]
    }
   ],
   "source": [
    " # TODO: Implement the random forest classifier here\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(max_depth = 100, class_weight = {0:1, 1:1, 2:1, 3:1, 4:1, 5:1,}, random_state=1100000)\n",
    "\n",
    "rf.fit(training_data_x,training_data_y)\n",
    "\n",
    "y_pred = rf.predict(test_data_x)\n",
    "\n",
    "print(accuracy_score(test_data_y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "65e10363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9776744186046512\n"
     ]
    }
   ],
   "source": [
    "# TODO: Perform some experiments here with different parameters of the random forest classifier. In the last line, print the accuracy with the best parameters.\n",
    "rf = RandomForestClassifier(max_depth = 100, class_weight = {0:1, 1:1, 2:1, 3:1, 4:1, 5:1,}, random_state=1100000)\n",
    "\n",
    "rf.fit(training_data_x,training_data_y)\n",
    "\n",
    "y_pred = rf.predict(test_data_x)\n",
    "\n",
    "print(accuracy_score(test_data_y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a738769a",
   "metadata": {},
   "source": [
    "TODO: Describe in one paragraph your experiments and evaluation of the random forest classifier. Consider things such as accuracy, training time, ease of tweaking of the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "0608dad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Honestly this classifier seems to be effected by the same variables as the logistic regression model. \n",
      "The accuracy is the same I imagine that it would run at a different time since the logistic regression \n",
      "program probably runs in a linear time where the rfc probably runs in log time.\n",
      "it has the same parameters as logistic regression for the most part\n"
     ]
    }
   ],
   "source": [
    "print(\"Honestly this classifier seems to be effected by the same variables as the logistic regression model. \\nThe accuracy is the same I imagine that it would run at a different time since the logistic regression \\nprogram probably runs in a linear time where the rfc probably runs in log time.\\nit has the same parameters as logistic regression for the most part\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c46a9f4",
   "metadata": {},
   "source": [
    "## P6 Bonus: Implement an AdaBoost classifer using sklearn (5 pts)\n",
    "\n",
    "Implement an AdaBoost classifier using sklearn https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "cc373403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9720930232558139\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement the adaboost classifier here\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada = AdaBoostClassifier(random_state=0)\n",
    "\n",
    "ada.fit(training_data_x, training_data_y)\n",
    "\n",
    "y_pred = ada.predict(test_data_x)\n",
    "\n",
    "print(accuracy_score(test_data_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "294dde6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8651162790697674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Seana\\anaconda3\\envs\\IrisProject\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:626: RuntimeWarning: overflow encountered in exp\n",
      "  sample_weight *= np.exp(\n",
      "c:\\Users\\Seana\\anaconda3\\envs\\IrisProject\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:506: UserWarning: Sample weights have reached infinite values, at iteration 1, causing overflow. Iterations stopped. Try lowering the learning rate.\n",
      "  return super().fit(X, y, sample_weight)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Perform some experiments here with different parametrizations of the adaboost classifier. In the last line, print the accuracy with the best parameters.\n",
    "\n",
    "ada = AdaBoostClassifier(learning_rate = 100, n_estimators = 100, random_state=1000)\n",
    "\n",
    "ada.fit(training_data_x, training_data_y)\n",
    "\n",
    "y_pred = ada.predict(test_data_x)\n",
    "\n",
    "print(accuracy_score(test_data_y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1dd74e",
   "metadata": {},
   "source": [
    "TODO: Describe in one paragraph your experiments and evaluation of the AdaBoost classifier. Consider things such as accuracy, training time, ease of tweaking of the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "6cc7e2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of adaboost has been lower than the accuracy of the other two classifiers I've been testing based on the instances I've observed.\n",
      "The parameters of adaboost is also different than the other two classifiers. learning_rate doesnt seem to effect the speed of the algorithm, \n",
      "however the number of estimators almost killed my pc\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy of adaboost has been lower than the accuracy of the other two classifiers I've been testing based on the instances I've observed.\\nThe parameters of adaboost is also different than the other two classifiers. learning_rate doesnt seem to effect the speed of the algorithm, \\nhowever the number of estimators almost killed my pc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('IrisProject')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "985aefe53c34445ca1822cf60effc5aff280a82e81fbde237d5a8a35ad828cda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
